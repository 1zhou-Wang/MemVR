![caseA](https://github.com/user-attachments/assets/8aa4155a-9741-4445-bb6a-d9f7500b20ef)# Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models
<!-- **Look Twice Before You Answer: Memory-Space Visual Retracing for Hallucination Mitigation in Multimodal Large Language Models** -->
This is the official implementation of MemVR!

<div style='display:flex; gap: 0.25rem; '>
<a href='LICENCE'><img src='https://img.shields.io/badge/License-Apache 2.0-g.svg'></a>
<a href='https://arxiv.org/pdf/2410.03577'><img src='https://img.shields.io/badge/Paper-PDF-red'></a>
<a href='https://www.google.com/'><img src='https://img.shields.io/badge/zhihu-Markdown-blue'></a>
</div>

## ğŸ”¥ Update
* [2024-10-7]: â­ï¸ Paper of MemVR uploaded. Check out [this link](https://arxiv.org/pdf/2410.03577) for details.
* [2024-11-14]: ğŸš€ğŸš€ Codes will be released after one month.

## ğŸ¯ Overview
![MemVR](figures/bigfig.png)

## ğŸ•¹ï¸ Usage


## ğŸ“Œ Examples
![Case1](figures/caseA.jpg)
*figure 5. Illustration of hallucination correction by our proposed VCD with two samples from LLaVA-Bench. Hallucinated objects from LVLM's regular decoding are highlighted in red.*

![Case2](figs/case_general.jpg)
*figure 8. More examples from LLaVA-Bench of our proposed VCD for enhanced general perception and recognition capacities.*

